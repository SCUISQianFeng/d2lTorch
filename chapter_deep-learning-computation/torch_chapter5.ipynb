{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5.1 层和块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-23T07:59:46.024934Z",
     "start_time": "2021-12-23T07:59:45.964969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1666,  0.1273,  0.0996, -0.0224,  0.2541, -0.0795,  0.2937, -0.1695,\n",
       "         -0.1536, -0.0460],\n",
       "        [ 0.1825,  0.0232,  0.1019, -0.0277,  0.3072, -0.0159,  0.2991, -0.2668,\n",
       "         -0.0080, -0.0412]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(in_features=20, out_features=256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(in_features=256, out_features=10))\n",
    "\n",
    "X = torch.rand(size=(2, 20))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0144,  0.1496,  0.0862,  0.0867,  0.3435, -0.0656,  0.0015,  0.1540,\n         -0.0204, -0.2431],\n        [ 0.0334,  0.1602,  0.0483,  0.1977,  0.0845, -0.0769, -0.0517,  0.1162,\n          0.0750, -0.1687]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 自定义块\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = nn.Linear(in_features=20, out_features=256)\n",
    "        self.out = nn.Linear(in_features=256, out_features=10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "X = torch.rand(size=(2, 20))\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2815,  0.0244,  0.2887,  0.0138, -0.3167,  0.0708,  0.0596,  0.0548,\n         -0.3138, -0.2818],\n        [ 0.4263, -0.1813,  0.2421,  0.1410, -0.2793,  0.1008,  0.0694,  0.0796,\n         -0.4309, -0.3870]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顺序块\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential, self).__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():  # D.values -> an object providing a view on D's value\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(in_features=20, out_features=256),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(in_features=256, out_features=10))\n",
    "X = torch.rand(size=(2, 20))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-0.2067, grad_fn=<SumBackward0>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 前向传播中执行代码\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FixedHiddenMLP, self).__init__()\n",
    "        self.rand_weight = torch.rand(size=(20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(in_features=20, out_features=20)\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "X = torch.rand(size=(2, 20))\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[ 0.1243, -0.0813, -0.1323, -0.1637, -0.2136, -0.1932, -0.4604, -0.1141,\n          -0.3389, -0.2385,  0.1201, -0.1476, -0.3179, -0.3248, -0.1174,  0.3162,\n           0.0389, -0.5190, -0.1210,  0.1834,  0.1344,  0.1521, -0.1481,  0.2967,\n           0.0743, -0.3023,  0.3846, -0.3553, -0.1025,  0.2429, -0.0315,  0.5640,\n           0.0583,  0.0910, -0.2686, -0.0109,  0.2129, -0.1305,  0.0647,  0.8903,\n          -0.5316,  0.1413,  0.1434,  0.0759, -0.1232, -0.1770,  0.2084, -0.1076,\n          -0.6032, -0.2984,  0.0267,  0.2979,  0.7035, -0.5143,  0.9181, -0.0983,\n           0.0969, -0.2187, -0.0411,  0.7130, -0.5817,  0.3987, -0.0164,  0.3483,\n           0.3313, -0.2712,  0.1693, -0.9545,  0.0856,  0.3903,  0.1812,  0.5665,\n           0.3584, -0.1828, -0.4873,  0.5350,  0.6609,  0.4909, -0.1759,  0.0279,\n           0.0620,  0.9492, -0.2923,  0.1423,  0.3196,  0.5844, -0.1143,  0.3250,\n          -0.1516,  0.4451, -0.8112, -0.4231, -0.1418,  0.8036,  0.1056, -0.3403,\n           0.1770, -0.2968, -0.4931,  0.0559, -0.9394,  0.7518,  0.2307,  0.3388,\n          -0.0921, -0.2156, -0.1595,  0.1422, -0.2507, -0.5770, -0.0760,  0.0273,\n           0.1209, -0.1001,  0.0647,  0.0977, -0.0594,  0.6181, -0.8051, -0.1615,\n           0.3502, -0.0614, -0.1716,  0.2591,  0.1595,  0.1047, -0.2817,  0.2494,\n          -0.0760,  0.4463, -0.5532,  0.2601,  0.6167,  0.6707, -0.1518,  0.1615,\n           0.2833, -0.2339,  0.3366,  0.9432, -0.4174,  0.7355,  0.1539,  0.3622,\n          -0.5610, -0.3826, -0.1399,  0.3238,  0.5123, -0.2012, -0.1900, -0.2755,\n          -0.1334, -0.1673, -0.3858,  0.6065, -0.1855, -0.3425,  0.3828, -0.7066,\n           0.5248,  0.2944,  0.6656,  0.0880, -0.2301,  0.0032, -0.3504,  0.1803,\n          -0.2707,  0.6736,  0.8873, -0.2547,  0.2384,  0.1969,  0.0237,  0.0073,\n           0.7623,  0.0278, -0.2919, -0.0810, -0.0288,  0.1331, -0.2399,  0.3955,\n          -0.1645,  0.1586,  0.0045, -0.0995, -0.7016,  0.0643,  0.4691, -0.1349,\n          -0.3049,  0.6424,  0.1780,  0.3384,  0.0130,  0.0682, -0.1160, -0.5612,\n          -0.3951, -0.1505,  0.5695, -0.4569,  0.4609,  0.4723,  0.3424, -0.0909,\n          -0.5380, -0.3220,  0.4153, -0.0077,  0.0688, -0.3348,  0.1913, -0.1888,\n           0.1580, -0.0291, -0.2553, -0.2915,  0.1571,  0.6341,  0.7333, -0.0936,\n           0.0485,  0.1256,  0.0907, -0.0729,  0.2304, -0.2463, -0.5920, -0.4548,\n           0.6703,  0.0600,  0.1183, -0.5856, -0.1730,  0.2052, -0.0874,  0.2445,\n           0.0128, -0.5362, -0.3321,  0.3994, -0.1467,  0.0325, -0.6004,  0.1093,\n          -0.0563,  0.5217,  0.5750, -0.5516,  0.2613,  0.0638,  0.3866, -0.1884],\n         [ 0.1388, -0.1094, -0.3115, -0.5360,  0.3953, -0.2341, -0.1733, -0.0368,\n          -0.2677, -0.1482,  0.2087,  0.0712, -0.3109, -0.1492, -0.2118,  0.1749,\n           0.2004, -0.5009,  0.2485,  0.3190, -0.2436, -0.5454,  0.0503,  0.3139,\n           0.2547, -0.3847,  0.0440, -0.3290, -0.5814, -0.1112,  0.0617,  0.2021,\n          -0.0628,  0.0187, -0.0059, -0.1505,  0.2345, -0.3233,  0.1871,  0.9381,\n          -0.5899,  0.1186, -0.2247,  0.1615,  0.2671, -0.2656, -0.0693, -0.2362,\n          -0.4862,  0.0435,  0.0157,  0.4967,  0.6246, -0.3788,  0.3364, -0.3348,\n           0.2873, -0.3046,  0.2372,  0.8598, -0.5524, -0.0927, -0.2606,  0.3138,\n           0.2379,  0.3542,  0.3535, -0.5316, -0.4208,  0.4937,  0.4531,  0.2782,\n           0.0455, -0.4101, -0.1153,  0.3139,  1.1169,  0.0697, -0.2729,  0.2783,\n           0.2477,  0.5625, -0.3542,  0.2248,  0.2144, -0.1028, -0.1169,  0.0511,\n          -0.2345, -0.3114, -0.5587, -0.1525, -0.1768,  0.2878,  0.1904, -0.6735,\n          -0.2268,  0.0479, -0.6783, -0.0052, -0.5138,  0.8088,  0.4127,  0.3086,\n          -0.3462, -0.2169, -0.4434, -0.0051, -0.1103, -0.2395, -0.0296, -0.0718,\n           0.1155, -0.4234,  0.1829, -0.1853, -0.4842,  0.2584, -0.2494, -0.4056,\n           0.3626,  0.1806, -0.3074,  0.1313, -0.2002,  0.2869, -0.0743, -0.4507,\n          -0.1758, -0.0780, -0.1041,  0.2122,  0.1254,  0.4618, -0.0307,  0.0098,\n           0.1999, -0.0668,  0.0604,  0.6679, -0.0741,  0.8219, -0.0523,  0.1345,\n          -0.2254,  0.1009,  0.0690, -0.0366,  0.3761, -0.1235,  0.0112, -0.5517,\n          -0.0129, -0.3098, -0.3419,  0.1761,  0.1853, -0.4304,  0.2372, -0.1884,\n           0.3495,  0.5593,  0.6051,  0.2930, -0.2795, -0.1995, -0.1467,  0.0063,\n          -0.0568,  0.3935,  0.7421, -0.2965, -0.1344, -0.5232,  0.0807,  0.1588,\n           0.3061,  0.0604,  0.1365,  0.0106, -0.2605,  0.1604,  0.1211,  0.0796,\n           0.0357,  0.2996, -0.0676, -0.0319, -0.5809, -0.3324,  0.3406, -0.0549,\n          -0.3406,  0.4851, -0.0118, -0.0741,  0.0735,  0.1906,  0.1085, -0.4978,\n          -0.4305, -0.1516,  0.0654, -0.1222,  0.7664,  0.4407,  0.1559, -0.1265,\n          -0.3232, -0.4485, -0.0300, -0.4937,  0.0470, -0.2607, -0.0423, -0.1002,\n          -0.1548,  0.5202,  0.1762, -0.0345,  0.4115,  0.1539,  0.3658, -0.0994,\n          -0.1208,  0.0996,  0.0955,  0.3514, -0.1438, -0.2425, -0.3095, -0.3794,\n           0.4557,  0.0116, -0.1628, -0.4671, -0.2361,  0.6984, -0.0074,  0.5426,\n           0.0357, -0.3079, -0.3919,  0.2139, -0.3873, -0.1102, -0.0177,  0.0720,\n          -0.3027,  0.4053, -0.1425, -0.4466, -0.1187, -0.2303,  0.0679, -0.0957]],\n        grad_fn=<AddmmBackward>),\n tensor([[0.1243, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.1201, 0.0000, 0.0000, 0.0000, 0.0000, 0.3162, 0.0389, 0.0000,\n          0.0000, 0.1834, 0.1344, 0.1521, 0.0000, 0.2967, 0.0743, 0.0000, 0.3846,\n          0.0000, 0.0000, 0.2429, 0.0000, 0.5640, 0.0583, 0.0910, 0.0000, 0.0000,\n          0.2129, 0.0000, 0.0647, 0.8903, 0.0000, 0.1413, 0.1434, 0.0759, 0.0000,\n          0.0000, 0.2084, 0.0000, 0.0000, 0.0000, 0.0267, 0.2979, 0.7035, 0.0000,\n          0.9181, 0.0000, 0.0969, 0.0000, 0.0000, 0.7130, 0.0000, 0.3987, 0.0000,\n          0.3483, 0.3313, 0.0000, 0.1693, 0.0000, 0.0856, 0.3903, 0.1812, 0.5665,\n          0.3584, 0.0000, 0.0000, 0.5350, 0.6609, 0.4909, 0.0000, 0.0279, 0.0620,\n          0.9492, 0.0000, 0.1423, 0.3196, 0.5844, 0.0000, 0.3250, 0.0000, 0.4451,\n          0.0000, 0.0000, 0.0000, 0.8036, 0.1056, 0.0000, 0.1770, 0.0000, 0.0000,\n          0.0559, 0.0000, 0.7518, 0.2307, 0.3388, 0.0000, 0.0000, 0.0000, 0.1422,\n          0.0000, 0.0000, 0.0000, 0.0273, 0.1209, 0.0000, 0.0647, 0.0977, 0.0000,\n          0.6181, 0.0000, 0.0000, 0.3502, 0.0000, 0.0000, 0.2591, 0.1595, 0.1047,\n          0.0000, 0.2494, 0.0000, 0.4463, 0.0000, 0.2601, 0.6167, 0.6707, 0.0000,\n          0.1615, 0.2833, 0.0000, 0.3366, 0.9432, 0.0000, 0.7355, 0.1539, 0.3622,\n          0.0000, 0.0000, 0.0000, 0.3238, 0.5123, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.6065, 0.0000, 0.0000, 0.3828, 0.0000, 0.5248, 0.2944,\n          0.6656, 0.0880, 0.0000, 0.0032, 0.0000, 0.1803, 0.0000, 0.6736, 0.8873,\n          0.0000, 0.2384, 0.1969, 0.0237, 0.0073, 0.7623, 0.0278, 0.0000, 0.0000,\n          0.0000, 0.1331, 0.0000, 0.3955, 0.0000, 0.1586, 0.0045, 0.0000, 0.0000,\n          0.0643, 0.4691, 0.0000, 0.0000, 0.6424, 0.1780, 0.3384, 0.0130, 0.0682,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.5695, 0.0000, 0.4609, 0.4723, 0.3424,\n          0.0000, 0.0000, 0.0000, 0.4153, 0.0000, 0.0688, 0.0000, 0.1913, 0.0000,\n          0.1580, 0.0000, 0.0000, 0.0000, 0.1571, 0.6341, 0.7333, 0.0000, 0.0485,\n          0.1256, 0.0907, 0.0000, 0.2304, 0.0000, 0.0000, 0.0000, 0.6703, 0.0600,\n          0.1183, 0.0000, 0.0000, 0.2052, 0.0000, 0.2445, 0.0128, 0.0000, 0.0000,\n          0.3994, 0.0000, 0.0325, 0.0000, 0.1093, 0.0000, 0.5217, 0.5750, 0.0000,\n          0.2613, 0.0638, 0.3866, 0.0000],\n         [0.1388, 0.0000, 0.0000, 0.0000, 0.3953, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.2087, 0.0712, 0.0000, 0.0000, 0.0000, 0.1749, 0.2004, 0.0000,\n          0.2485, 0.3190, 0.0000, 0.0000, 0.0503, 0.3139, 0.2547, 0.0000, 0.0440,\n          0.0000, 0.0000, 0.0000, 0.0617, 0.2021, 0.0000, 0.0187, 0.0000, 0.0000,\n          0.2345, 0.0000, 0.1871, 0.9381, 0.0000, 0.1186, 0.0000, 0.1615, 0.2671,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0435, 0.0157, 0.4967, 0.6246, 0.0000,\n          0.3364, 0.0000, 0.2873, 0.0000, 0.2372, 0.8598, 0.0000, 0.0000, 0.0000,\n          0.3138, 0.2379, 0.3542, 0.3535, 0.0000, 0.0000, 0.4937, 0.4531, 0.2782,\n          0.0455, 0.0000, 0.0000, 0.3139, 1.1169, 0.0697, 0.0000, 0.2783, 0.2477,\n          0.5625, 0.0000, 0.2248, 0.2144, 0.0000, 0.0000, 0.0511, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.2878, 0.1904, 0.0000, 0.0000, 0.0479, 0.0000,\n          0.0000, 0.0000, 0.8088, 0.4127, 0.3086, 0.0000, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.1155, 0.0000, 0.1829, 0.0000, 0.0000,\n          0.2584, 0.0000, 0.0000, 0.3626, 0.1806, 0.0000, 0.1313, 0.0000, 0.2869,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2122, 0.1254, 0.4618, 0.0000,\n          0.0098, 0.1999, 0.0000, 0.0604, 0.6679, 0.0000, 0.8219, 0.0000, 0.1345,\n          0.0000, 0.1009, 0.0690, 0.0000, 0.3761, 0.0000, 0.0112, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.1761, 0.1853, 0.0000, 0.2372, 0.0000, 0.3495, 0.5593,\n          0.6051, 0.2930, 0.0000, 0.0000, 0.0000, 0.0063, 0.0000, 0.3935, 0.7421,\n          0.0000, 0.0000, 0.0000, 0.0807, 0.1588, 0.3061, 0.0604, 0.1365, 0.0106,\n          0.0000, 0.1604, 0.1211, 0.0796, 0.0357, 0.2996, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.3406, 0.0000, 0.0000, 0.4851, 0.0000, 0.0000, 0.0735, 0.1906,\n          0.1085, 0.0000, 0.0000, 0.0000, 0.0654, 0.0000, 0.7664, 0.4407, 0.1559,\n          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0470, 0.0000, 0.0000, 0.0000,\n          0.0000, 0.5202, 0.1762, 0.0000, 0.4115, 0.1539, 0.3658, 0.0000, 0.0000,\n          0.0996, 0.0955, 0.3514, 0.0000, 0.0000, 0.0000, 0.0000, 0.4557, 0.0116,\n          0.0000, 0.0000, 0.0000, 0.6984, 0.0000, 0.5426, 0.0357, 0.0000, 0.0000,\n          0.2139, 0.0000, 0.0000, 0.0000, 0.0720, 0.0000, 0.4053, 0.0000, 0.0000,\n          0.0000, 0.0000, 0.0679, 0.0000]], grad_fn=<ReluBackward0>),\n tensor([[ 0.1297, -0.0841,  0.3048,  0.1678, -0.0166,  0.3203,  0.2718, -0.0291,\n          -0.0046, -0.1570],\n         [ 0.0427, -0.0533,  0.1483, -0.0786,  0.0087,  0.3173,  0.0982,  0.2178,\n           0.1724, -0.1216]], grad_fn=<AddmmBackward>)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quiz\n",
    "\n",
    "class MySequentialQuiz(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(MySequentialQuiz, self).__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = []\n",
    "        for block in self._modules.values():  # D.values -> an object providing a view on D's value\n",
    "            X = block(X)\n",
    "            out.append(X)\n",
    "        return out\n",
    "\n",
    "net = MySequentialQuiz(nn.Linear(in_features=20, out_features=256),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Linear(in_features=256, out_features=10))\n",
    "X = torch.rand(size=(2, 20))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2543],\n        [0.2539]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.2 参数管理\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(in_features=4, out_features=8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(in_features=8, out_features=1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.0842, -0.1992,  0.1334, -0.0959, -0.0766,  0.2101,  0.2044, -0.0852]])), ('bias', tensor([0.2948]))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(net[2].state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0842, -0.1992,  0.1334, -0.0959, -0.0766,  0.2101,  0.2044, -0.0852]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2948], requires_grad=True)\n",
      "tensor([0.2948])\n",
      "[0.29475138]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.29475137591362]\n",
      "<class 'list'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 目标参数\n",
    "print(net[2].weight)\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(net[2].bias.data.cpu().numpy())\n",
    "print(type(net[2].bias.data.cpu().numpy()))\n",
    "print(net[2].bias.data.cpu().numpy().tolist())\n",
    "print(type(net[2].bias.data.cpu().numpy().tolist()))\n",
    "print(net[2].weight.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[ 0.0090, -0.1568, -0.3177, -0.2443],\n",
      "        [-0.0004,  0.0025,  0.3071,  0.0847],\n",
      "        [ 0.2652, -0.3415,  0.2819, -0.2263],\n",
      "        [-0.2020, -0.2395, -0.0179, -0.0793],\n",
      "        [ 0.2286,  0.0326,  0.3720,  0.2633],\n",
      "        [ 0.1730, -0.0229,  0.3710, -0.0860],\n",
      "        [-0.2640,  0.3199, -0.3763,  0.2332],\n",
      "        [ 0.1525, -0.0613,  0.0304,  0.1764]], requires_grad=True)) ('bias', Parameter containing:\n",
      "tensor([ 0.0722, -0.4419,  0.4681,  0.4439, -0.3091, -0.4832, -0.4247,  0.4015],\n",
      "       requires_grad=True))\n",
      "('0.weight', Parameter containing:\n",
      "tensor([[ 0.0090, -0.1568, -0.3177, -0.2443],\n",
      "        [-0.0004,  0.0025,  0.3071,  0.0847],\n",
      "        [ 0.2652, -0.3415,  0.2819, -0.2263],\n",
      "        [-0.2020, -0.2395, -0.0179, -0.0793],\n",
      "        [ 0.2286,  0.0326,  0.3720,  0.2633],\n",
      "        [ 0.1730, -0.0229,  0.3710, -0.0860],\n",
      "        [-0.2640,  0.3199, -0.3763,  0.2332],\n",
      "        [ 0.1525, -0.0613,  0.0304,  0.1764]], requires_grad=True)) ('0.bias', Parameter containing:\n",
      "tensor([ 0.0722, -0.4419,  0.4681,  0.4439, -0.3091, -0.4832, -0.4247,  0.4015],\n",
      "       requires_grad=True)) ('2.weight', Parameter containing:\n",
      "tensor([[ 0.0842, -0.1992,  0.1334, -0.0959, -0.0766,  0.2101,  0.2044, -0.0852]],\n",
      "       requires_grad=True)) ('2.bias', Parameter containing:\n",
      "tensor([0.2948], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问所有参数\n",
    "# print([module for module in net.named_modules() ])\n",
    "# modules = [module for module in net.named_modules() ]\n",
    "# print(isinstance(modules[0], nn.Module))\n",
    "print(*[(name, param) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param) for name, param in net.named_parameters()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1579],\n        [0.1579]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从嵌套层中收集参数\n",
    "\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(in_features=4, out_features=8), nn.ReLU(),\n",
    "                        nn.Linear(in_features=8, out_features=4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(name=f'block-{i}', module=block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(in_features=4, out_features=1))\n",
    "rgnet(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block-0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block-1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block-2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block-3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4061, -0.4364,  0.2558,  0.2830,  0.1445, -0.2186, -0.0677,  0.0837])\n"
     ]
    }
   ],
   "source": [
    "print(rgnet[0][1][0].bias.data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0030,  0.0002,  0.0107, -0.0070],\n",
      "        [-0.0074,  0.0075,  0.0142,  0.0101],\n",
      "        [ 0.0020, -0.0098,  0.0043, -0.0068],\n",
      "        [-0.0064,  0.0032,  0.0025,  0.0009],\n",
      "        [-0.0056, -0.0004, -0.0136,  0.0025],\n",
      "        [-0.0182, -0.0191,  0.0078,  0.0153],\n",
      "        [-0.0024, -0.0180,  0.0023,  0.0149],\n",
      "        [ 0.0035, -0.0025, -0.0093,  0.0038]])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 内置初始化\n",
    "def init_normal(m: nn.Module):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "print(net[0].weight.data)\n",
    "print(net[0].bias.data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1096, -0.0094, -0.2263,  0.5263],\n",
      "        [-0.5645, -0.0517,  0.6106,  0.1674],\n",
      "        [-0.4088,  0.5038,  0.3854,  0.0969],\n",
      "        [-0.5079,  0.1104, -0.4637,  0.5731],\n",
      "        [-0.3645, -0.6534,  0.6860,  0.6882],\n",
      "        [-0.2698, -0.6082,  0.0899, -0.6315],\n",
      "        [-0.4236,  0.1624,  0.4415, -0.1981],\n",
      "        [-0.5675,  0.2436,  0.3304, -0.1649]])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 内置初始化-常量初始化\n",
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(tensor=m.weight, val=42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 自定义初始化\n",
    "def init_my(m: nn.Module):\n",
    "    print('Init', *[(name, param.shape) for name, param in m.name_parameters()][0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9a35f556",
   "language": "python",
   "display_name": "PyCharm (ml-trainee)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}